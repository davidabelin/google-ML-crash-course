{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " car_data_with_clustering_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT_bZ9E0ZWaN"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Let's start by importing our dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhVcUJHtTZ8g"
      },
      "source": [
        "# DERIVED FROM :\n",
        "# https://colab.research.google.com/github/google/eng-edu/blob/master/ml/fe/exercises/intro_to_modeling.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "wZ_T2SgDVKUH",
        "outputId": "c8292409-62de-4278-e09a-0d2f78b21658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "%reset -f\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rd\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Set pandas output display to have one digit for decimal places and limit it to\n",
        "# printing 15 rows.\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "pd.options.display.max_rows = 15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Y38V73EgVYwt"
      },
      "source": [
        "# Provide the names for the columns since the CSV file \n",
        "# with the data does not have a header row.\n",
        "feature_names = ['symboling', 'normalized-losses', 'make', 'fuel-type',\n",
        "        'aspiration', 'num-doors', 'body-style', 'drive-wheels',\n",
        "        'engine-location', 'wheel-base', 'length', 'width', 'height', 'weight',\n",
        "        'engine-type', 'num-cylinders', 'engine-size', 'fuel-system', 'bore',\n",
        "        'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg',\n",
        "        'highway-mpg', 'price']\n",
        "\n",
        "# ORIGINAL DATA WITH '?'\n",
        "car_data = pd.read_csv('https://storage.googleapis.com/mledu-datasets/cars_data.csv',\n",
        "                        sep=',', names=feature_names, header=None, encoding='latin-1')\n",
        "\n",
        "# FIND ROWS WITH BLANKS\n",
        "blank_rows = list([])\n",
        "blank_cols = list([])\n",
        "nl_blank_rows = list([])\n",
        "\n",
        "for label in car_data[feature_names]:\n",
        "    for key,val in enumerate(car_data[label]):\n",
        "        if str(val) == '?':\n",
        "            if str(label) == 'num-doors':        # FILL NUM_DOORS BLANKS\n",
        "                car_data.at[key,label] = 'none'\n",
        "                print(\"Replaced num-doors blanks with 'none'\")\n",
        "            else:\n",
        "                if str(label) == 'normalized-losses':\n",
        "                    #print('Label is',label)\n",
        "                    nl_blank_rows += [key]\n",
        "                else:\n",
        "                    print('Row', key, 'in [',label,'] has blanks.')\n",
        "                    if not(key in blank_rows):    \n",
        "                        blank_rows += [key]\n",
        "                    if not(label in blank_cols):\n",
        "                        blank_cols += [label]\n",
        "\n",
        "blank_rows.sort()\n",
        "blank_data = pd.DataFrame(car_data.iloc[blank_rows])\n",
        "car_data = pd.DataFrame(car_data.drop(blank_rows))\n",
        "\n",
        "# RANDOMIZE INDEX\n",
        "blank_data.reset_index(drop=True,inplace=True)\n",
        "print(\"Loaded and reindexed test_data.\")\n",
        "car_data.reset_index(drop=True,inplace=True)\n",
        "car_data = car_data.reindex(np.random.permutation(np.arange(0,len(car_data))),copy=True)    \n",
        "print(\"Loaded and reindexed car_data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueQAjoKBEzsA"
      },
      "source": [
        "test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kaF1ZSG_wDz"
      },
      "source": [
        "norm_data = pd.DataFrame(car_data,columns=list(set(numeric_feature_names) - set(blank_cols)),copy=True)\n",
        "norm_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "VsOUrVozoe9u"
      },
      "source": [
        "# DEFINE VALUES\n",
        "\n",
        "EPSILON = 0.000001  #don't divide by zero\n",
        "LABEL = 'normalized-losses' #to start with\n",
        "PRED = [] #collect predictions\n",
        "# AVOID OVER-NORMALIZING!\n",
        "ZDONE = False\n",
        "LOGDONE = False\n",
        "NORMDONE = False\n",
        "BUCKDONE = False\n",
        "SCADONE = False\n",
        "CLIPPED = False\n",
        "\n",
        "numeric_feature_names = ['symboling','normalized-losses', 'wheel-base',\n",
        "        'length', 'width', 'height', 'weight', 'engine-size', 'horsepower',\n",
        "        'peak-rpm', 'city-mpg', 'highway-mpg', 'bore', 'stroke',\n",
        "         'compression-ratio','price']\n",
        "categorical_feature_names = list(set(feature_names) - set(numeric_feature_names) )#- set([LABEL]))\n",
        "\n",
        "log_feature_names = ['wheel-base', 'horsepower', 'city-mpg', 'highway-mpg']\n",
        "bucket_list = ['symboling','compression-ratio']\n",
        "norm_feature_names = ['length', 'width', 'height', 'weight', 'engine-size',\n",
        "                      'peak-rpm','stroke', 'bore', 'price']\n",
        "\n",
        "#WORKING SETS TO PREPARE FOR USE IN TRAINING\n",
        "x_df = pd.DataFrame( car_data[list(set(numeric_feature_names) - set(['normalized-losses']))] , dtype=float)\n",
        "b_df = pd.DataFrame( blank_data[list(set(numeric_feature_names) - set(['normalized-losses']))]  )\n",
        "#y_series = pd.Series( 10*(car_data[LABEL] - car_data[LABEL].min()) / \n",
        "#                    (EPSILON + car_data[LABEL].max() - car_data[LABEL].min()), \n",
        "#                    dtype=float)\n",
        "\n",
        "# The correct count will pass these assert statements.\n",
        "assert len(numeric_feature_names) == 16  #INCLUDING LABEL\n",
        "assert len(categorical_feature_names) == 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUKWRfAwQJ0Z"
      },
      "source": [
        "#car_data[list(set(numeric_feature_names)-set(label_feature_names)-set(LABEL))]\n",
        "print(LABEL)\n",
        "print(blank_cols + [LABEL])\n",
        "print(numeric_feature_names)\n",
        "print(list(set(numeric_feature_names) - set(blank_cols + [LABEL])))#(label_feature_names) - set(LABEL)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wiqvNTaWNRA"
      },
      "source": [
        "# DEFINE SOME FUNCTIONS\n",
        "\n",
        "#SCALE INPUT DOWN TO (0,size)\n",
        "def scale_to (feature_list, size=10): #size the max scaled value\n",
        "    #if not(SCADONE): \n",
        "        for feature_name in feature_list: \n",
        "            fnmax = x_df[feature_name].max()\n",
        "            fnmin = x_df[feature_name].min()      \n",
        "            x_df[feature_name] = size*(x_df[feature_name] - fnmin) / (fnmax - fnmin)\n",
        "            print(feature_name,\" scaled to (\",x_df[feature_name].min(),\",\",x_df[feature_name].max(),\")\") \n",
        "        #SCADONE = True\n",
        "    #else:\n",
        "    #    print(\"~~~ Data has already been clipped. ~~~\")\n",
        "\n",
        "\n",
        "# CLIP TO NUMBER OF STANDARD DEVIATIONS FROM MIN, MAX\n",
        "def clip_by (feature_list,sigmas=1): #sigmas is how many SD's from min, max to clip\n",
        "    #if not(CLIPPED):\n",
        "        for feature_name in feature_list:\n",
        "            fnmax = x_df[feature_name].max()\n",
        "            fnmin = x_df[feature_name].min() \n",
        "            fnstd = x_df[feature_name].std() \n",
        "            for key, val in enumerate(x_df[feature_name]):\n",
        "                if val > fnmax - sigmas*fnstd:              \n",
        "                    x_df[feature_name][key] = fnmax - sigmas*fnstd\n",
        "                if val < fnmin + sigmas*fnstd: \n",
        "                    x_df[feature_name][key] = fnmin + sigmas*fnstd\n",
        "            print(feature_name, \"clipped to (\",fnmin + sigmas*fnstd,\",\",fnmax - sigmas*fnstd,\") from (\",fnmin,\",\",fnmax,\")\")\n",
        "        #CLIPPED = True\n",
        "    #else:\n",
        "    #    print(\"~~~ Data has already been clipped. ~~~\")\n",
        "# END CLIP_BY\n",
        "\n",
        "#LOG NORM\n",
        "def log_norm (feature_list,LOGDONE):\n",
        "    if not(LOGDONE):\n",
        "        for feature_name in feature_list:\n",
        "            for key, val in enumerate(x_df[feature_name]):\n",
        "                if val <= 0.0:\n",
        "                    print(feature_name,key,\" is equal to\",val,\"!\")\n",
        "                    #x_df[feature_name][key] = np.log(val+EPSILON)\n",
        "                else:\n",
        "                    x_df.at[key,feature_name] = np.log(val)\n",
        "            print(feature_name,\"log-scaled.\")\n",
        "        LOGDONE = True\n",
        "    else:\n",
        "        print(\"~~~ Data has already been log-normed. ~~~\")\n",
        "\n",
        "# Z NORMALIZE\n",
        "def Z_norm (feature_list):\n",
        "    #if not(ZDONE):        \n",
        "        for feature_name in feature_list:\n",
        "            fnstd = x_df[feature_name].std()\n",
        "            fnmean = x_df[feature_name].mean()\n",
        "            x_df[feature_name] = (x_df[feature_name] - fnmean)/(fnstd)\n",
        "            print(feature_name,\" scaled to (\",x_df[feature_name].min(),\",\",x_df[feature_name].max(),\")\")\n",
        "    #   ZDONE = True\n",
        "    #else:\n",
        "    #    print(\"~~~ Data has already been Z-normed. ~~~\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjZIZHEFNxvF"
      },
      "source": [
        "# VISUALIZATION WITH PYPLOT\n",
        "def scatter_plot_grid(feature_list, with_predictions=False):\n",
        "\n",
        "    # SCATTER_PLOT PLOTS A SCATTER PLOT FOR EACH FEATURE\n",
        "    def scatter_plot(axis, x_axis_fn):\n",
        "        axis.set_ylabel(LABEL)\n",
        "        axis.set_xlabel(x_axis_fn)\n",
        "        # PLOT TRAINING SET IN GREY          \n",
        "        axis.scatter(x_df[x_axis_fn], y_series, c='grey') #Y_SERIES IS SCALED SET OF LABELS\n",
        "        \n",
        "        if with_predictions:\n",
        "            # PLOT PRICES PREDICTED FROM TEST SET DATA IN ORANGE\n",
        "            axis.scatter(test_df[x_axis_fn], predictions, c='orange')\n",
        "    # END OF DEFINE SCATTER_PLOT\n",
        "\n",
        "    if with_predictions:       \n",
        "        predictions = [  x['predictions'][0] for x in est.predict(predict_input_fn)   ]\n",
        "\n",
        "    #ARRANGE SUBPLOTS AS GRID\n",
        "    num_cols = 4\n",
        "    num_rows = int(math.ceil(len(feature_list)/float(num_cols)))\n",
        "    f, axarr = plt.subplots(num_rows, num_cols)\n",
        "    s = 4.5\n",
        "    f.set_size_inches(num_cols*s, num_rows*s)\n",
        "\n",
        "    # ADD EACH FEATURE PLOT TO THE GRID\n",
        "    for i, fn in enumerate(feature_list):\n",
        "        axis = axarr[int(i/num_cols), i%num_cols]\n",
        "        scatter_plot(axis, fn)\n",
        "\n",
        "    # CALL PYPLOT TO DISPLAY IT\n",
        "    plt.show()\n",
        "\n",
        "# END OF DEFINE SCATTER_PLOT_GRID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ULYVRMIr5B"
      },
      "source": [
        "# DISPLAY DATA SO FAR\n",
        "scatter_plot_grid(numeric_feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiQceDWMnmru"
      },
      "source": [
        "#FUNCTION PLAYGROUND\n",
        "\n",
        "def lorm (n):\n",
        "    return tf.log(n) #TF!\n",
        "\n",
        "def loggit(fc):     #TF TENSOR --> PD SERIES\n",
        "    #print(\"loggit\")\n",
        "    tx = pd.Series( fc ,copy=True,dtype=np.float32)\n",
        "    #BACK TO TENSER?\n",
        "    return np.log(tx) #NP!\n",
        "\n",
        "def norm (z,mx,mn,sf=10):      #TF --> PD  --> TF\n",
        "    tx = pd.Series(z,copy=True,dtype=np.float32) \n",
        "    tx = sf*((z - mn) / (mx - mn))\n",
        "    #print(sf,\"*((z \",\"-\",mn,\") / (\",mx,\"-\",mn,\"))\")\n",
        "    return tf.cast(tx,dtype=tf.float32) \n",
        "\n",
        "#ZCLIP TF --> ??\n",
        "def zclip (fc,mx,mn,sd,sigmas=1): #sigmas is how many SD's from min, max to clip\n",
        "    print(\"zclip\")\n",
        "    tx = sigclip( fc ,mx,mn,sd,sigmas) \n",
        "    return tx\n",
        "\n",
        "#SIGCLIP TF --> ??\n",
        "def sigclip(fc,mx,mn,sd,sf=1):\n",
        "    print(\"sigclip\")\n",
        "    #tf.greater()\n",
        "    for key, val in enumerate(tx):\n",
        "        if val > mx - sf*sd:              \n",
        "            tx[key] = mx - sf*sd\n",
        "        if val < mn + sf*sd: \n",
        "            tx[key] = mn + sf*sd\n",
        "    return tx\n",
        "\n",
        "#ana = np.arange(10,100,step=10)\n",
        "ary = pd.Series(np.arange(10,110,10,dtype=np.float32))\n",
        "tary = tf.convert_to_tensor(ary)\n",
        "\n",
        "print(\"series ary\\n\",ary)\n",
        "#ala = lambda zoo: zoo*zoo\n",
        "foo = lorm(tary) #, ary.max() , ary.min() )\n",
        "#foo = zclip( ary, np.max(ary) , np.min(ary), np.std(ary) )\n",
        "print(\"lorm Tary\\n\",foo)\n",
        "#print(\"series from tary\\n\",list(tary))\n",
        "#print(foo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKJr3UDGsOmv"
      },
      "source": [
        "model_feature_columns = [] #LAYERS TO FEED TO ESTIMATOR\n",
        "\n",
        "# FOR NORMAL NUMERIC LAYERS\n",
        "for fn in norm_feature_names:  \n",
        "    fnmin = np.min(x_df[fn])\n",
        "    fnmax = np.max(x_df[fn])\n",
        "    fnstd = np.std(x_df[fn]) \n",
        "    #tx = pd.Series(10*(x_df(fn) - fnmin)/(fnmax - fnmin),dtype=np.float32,copy=True)\n",
        "    tensor_column = tf.feature_column.numeric_column(fn,\n",
        "                             normalizer_fn= lambda x: 10*(x - fnmin)/(fnmax - fnmin)) \n",
        "    model_feature_columns.append(tensor_column)\n",
        "print(\"Scaled/Normalized numeric layers\")\n",
        "\n",
        "# FOR LOG NUMERIC LAYERS\n",
        "for fn in log_feature_names:\n",
        "    tensor_column = tf.feature_column.numeric_column(fn,\n",
        "                             normalizer_fn=lambda nf: tf.log(nf))\n",
        "    model_feature_columns.append(tensor_column)\n",
        "print(\"Normalized log layers.\")\n",
        "\n",
        "# FOR BUCKET-LIST LAYERS\n",
        "for fn in bucket_list:\n",
        "    fnmin = min(x_df[fn])\n",
        "    fnmax = max(x_df[fn])\n",
        "    fnstd = x_df[fn].std()   \n",
        "    tensor_column = tf.feature_column.numeric_column(fn,\n",
        "                             normalizer_fn= \n",
        "                             lambda x: 10*(x - fnmin)/(fnmax - fnmin))     \n",
        "    if fn == 'symboling':\n",
        "        feature_bucket_step_size = 1\n",
        "    if fn == 'compression-ratio':\n",
        "        feature_bucket_step_size = (fnmax-fnmin)/2\n",
        "    else:\n",
        "        feature_bucket_step_size = (fnmax-fnmin)/12                                            \n",
        "    bucket_boundaries = list(np.arange(fnmin, fnmax, \n",
        "                                       feature_bucket_step_size))\n",
        "    new_column = tf.feature_column.bucketized_column(tensor_column, bucket_boundaries)\n",
        "    model_feature_columns.append(new_column)\n",
        "    \n",
        "print(\"Scaled/Normalized bucket-list layers.\")\n",
        "\n",
        "print('model_feature_columns:\\n')\n",
        "model_feature_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHqWlz_1pGBK"
      },
      "source": [
        "# INPUT FUNCTIONS TO SEND TO ESTIMATOR\n",
        "\n",
        "batch_size = 12\n",
        "\n",
        "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=None,\n",
        "    shuffle=True  )\n",
        "\n",
        "eval_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=x_df,\n",
        "    y=y_series,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    x=test_df,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C7YZDmHclA5"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HETI8vHrwG7"
      },
      "source": [
        "# ESTIMATOR\n",
        "\n",
        "est = tf.estimator.DNNRegressor(\n",
        "    feature_columns=model_feature_columns, \n",
        "    hidden_units=[64],\n",
        "    optimizer=tf.train.AdagradOptimizer(learning_rate=0.01) )\n",
        "\n",
        "# TRAIN\n",
        "num_print_statements = 20\n",
        "num_training_steps = 10000\n",
        "for _ in range(num_print_statements):\n",
        "  est.train(train_input_fn, steps=num_training_steps // num_print_statements)\n",
        "  scores = est.evaluate(eval_input_fn)\n",
        "  print('SCORES:', scores,\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGSXwX2fju1N"
      },
      "source": [
        "scatter_plot_grid(numeric_feature_names,True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}